{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6e1840f",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Neural Network\n",
    "##### Multi-class sentiment analysis NN using DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "2e267d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all required dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Importing (TODO) for metrics evaluation\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "89e68175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-emotion classification\n",
    "class MultiLabelSentimentClassifier(nn.Module):\n",
    "    def __init__(self, model_name, num_labels):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(model_name)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.classifier(outputs.last_hidden_state[:, 0, :])\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b9db926c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sentiment dataset using DistilBERT\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3812c0a3",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ba45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: texts, labels = [...], [...]\n",
    "# labels should be one-hot encoded, e.g. [0,0,1,0,0]\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# dataset = SentimentDataset(texts, labels, tokenizer, max_length=128)\n",
    "# loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(loader, desc=f\"Epoch {epoch+1}\", colour=\"blue\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173e2ab7",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f8ebdf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, sentence, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoding = tokenizer(\n",
    "            sentence,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=128,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].to(device)\n",
    "        attention_mask = encoding['attention_mask'].to(device)\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
    "        return (probs >= threshold).astype(int), probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3a956c",
   "metadata": {},
   "source": [
    "#### DATASET INFO\n",
    "Dataset from Kaggle - [*Emotion Dataset for Emotion Recognition Tasks*](https://www.kaggle.com/datasets/parulpandey/emotion-dataset?select=training.csv)\n",
    "\n",
    "**Parameters**  \n",
    "Columns - Text | Label  \n",
    "Size = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb77cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Emotions\n",
    "emotions = [\"sadness\", \"joy\", \"love\", \"anger\", \"fear\", \"UNK\"]\n",
    "num_labels = len(emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encode integer labels in dataset to one-hot\n",
    "# encoded labels for training/inferencing\n",
    "def labels_to_multihot(label, num_classes):\n",
    "    vec = [0] * num_classes\n",
    "    vec[int(label)] = 1\n",
    "    return vec\n",
    "\n",
    "df = pd.read_csv(\"../data-prep/Base_data/training.csv\")\n",
    "\n",
    "# Extract 7680 data samples (0.48*16000) for training data set\n",
    "# This creates exactly 60 batches with batch_size=128\n",
    "train_df = df.sample(frac=0.48, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfa8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 7680\n",
      "Sample text: i feel furious at myself for being so pathetic furious at her for various reasons\n",
      "Sample  label (one-hot): [0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Convert .csv into texts/labels for\n",
    "texts = train_df['text'].tolist()\n",
    "labels = [labels_to_multihot(l, num_labels) for l in df[\"label\"]]\n",
    "\n",
    "print(f\"Number of training samples: {len(texts)}\")\n",
    "idx = 5000\n",
    "print(\"Sample text:\", texts[idx])\n",
    "print(\"Sample  label (one-hot):\", labels[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dce549e",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ba4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size\n",
    "batch_size = 128\n",
    "# Threshold for final inferencing per logit\n",
    "prob_threshold = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4ad727",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b31a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer, dataset, dataloader\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "dataset = SentimentDataset(texts, labels, tokenizer, max_length=32)\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, loss\n",
    "model = MultiLabelSentimentClassifier(model_name, num_labels).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb71a9b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "01b61748",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:12<00:00,  1.21s/it]\n",
      "Epoch 1: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:12<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.4415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:33<00:00,  1.57s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.4091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:30<00:00,  1.50s/it]\n",
      "Epoch 3: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:30<00:00,  1.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.4078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:31<00:00,  1.52s/it]\n",
      "Epoch 4: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:31<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.4060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:28<00:00,  1.47s/it]\n",
      "Epoch 5: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:28<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.3996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:34<00:00,  1.57s/it]\n",
      "Epoch 6: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:34<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.3830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:31<00:00,  1.53s/it]\n",
      "Epoch 7: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:31<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.3487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:13<00:00,  1.22s/it]\n",
      "Epoch 8: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:13<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.2977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:15<00:00,  1.27s/it]\n",
      "Epoch 9: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:15<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.2442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|\u001b[34m██████████\u001b[0m| 60/60 [01:11<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.1978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train for 'n' epochs\n",
    "num_epochs = 10\n",
    "train(model, loader, optimizer, criterion, device, epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e28812b",
   "metadata": {},
   "source": [
    "### Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1ebdd47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: What a lovely day\n",
      "Probabilities > 0.5:  [0 0 0 0 1 1]\n",
      "Predicted labels: ['fear', 'UNK']\n",
      "Probabilities: [0.4293264  0.38216183 0.4317745  0.44819912 0.5323816  0.6052199 ]\n"
     ]
    }
   ],
   "source": [
    "# Predict emotions for a new sentence\n",
    "sentence = \"What a lovely day\"\n",
    "pred, probs = predict(model, tokenizer, sentence, device, threshold=prob_threshold)\n",
    "print(\"Sentence:\", sentence)\n",
    "print(f\"Probabilities > {prob_threshold}: \", pred)\n",
    "print(\"Predicted labels:\", [emotions[i] for i, v in enumerate(pred) if v])\n",
    "print(\"Probabilities:\", probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37eabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select half the data for training\n",
    "# Set a random seed for reproducibility if desired\n",
    "train_df = df.sample(frac=0.5, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Prepare texts and multi-hot labels from the sampled data\n",
    "texts = train_df['text'].tolist()\n",
    "labels = [labels_to_multihot(l, len(emotions)) for l in train_df['label']]\n",
    "\n",
    "print(f\"Number of training samples: {len(texts)}\")\n",
    "print(\"Sample text:\", texts[0])\n",
    "print(\"Sample label (multi-hot):\", labels[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
